| Название | Год | Автор | Ссылка | Краткое содержание |
| -------- |---- | ----- | ------ | ---- |
|Algorithms for Hyper-Parameter Optimization|2011|J. Bergstra, et al.|[link](https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)|Два метода на основе SMBO и критерия Expected Improvement: GP - моделирование $p(y\|x)$ с помощью гауссовского процесса; TPE - моделирование $p(x\|y)$ и $p(x)$, построение распределений гиперпараметров и сэмплирование новых, максимизирующих $\frac{l(x)}{g(x)}$|
|Sequential Model-Based Optimization for General Algorithm Configuration|2011|F. Hutter, et al.|[link](https://ml.informatik.uni-freiburg.de/wp-content/uploads/papers/11-LION5-SMAC.pdf)|Описывают SMBO, вводят процедуру Intensify, которая выбирает оптимальный HPC на основе множества датасетов, представляют SMAC - SMBO с RF в качестве суррогата для поддержи категориальных переменных|
|Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization|2016|Li, Lisha, et al.|[link](https://arxiv.org/abs/1603.06560)|Запускает Succesive Halving много раз, постоянно увеличивая бюджет для лучших моделей|
|Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges|2021|B. Bischl, et al.|[link](https://arxiv.org/abs/2107.05847)|Обзорная статья о текущих методах HPO, все алгоритмы формально описаны. Рассказано о текущих проблемах в области HPO, таких как overtuning и отсутствие регуляризации|
|Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets|2016|A. Klein, et al.|[link](https://arxiv.org/abs/1605.07079)|Добавили к пространству гиперпараметров новый параметр $s \in (0, 1]$, означающий долю размера датасета. Таким образом, используя SMBO с GP, можно обучать модель на меньшем датасете и экстраполировать значение ошибки на валидации для $s=1$|
|Scalable Bayesian Optimization Using Deep Neural Networks|2015|J. Snoek, et al.|[link](https://arxiv.org/pdf/1502.05700.pdf)|SMBO, суррогат - NN + Bayesian Linear Regression. Быстрее чем GP (рост линейный от количества тестов)|
|Optimization of deep neural networks: a survey and unified taxonomy|2020|El-Ghazali Talbi|[link](https://inria.hal.science/hal-02570804v2/document)|Обзорная статья, систематизация знаний о HPO, NAS и AutoDNN|
|BOHB: Robust and Efficient Hyperparameter Optimization at Scale|2018|S. Falkner, et al.|[link](https://arxiv.org/abs/1807.01774)|Комбинация TPE и идеи HyperBand: HPC предлагаются с помощью TPE, $\lambda_{fid}$ повышается для новых HPC|
|Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces|1997|R. Storn, K. Price |[link](https://link.springer.com/article/10.1023/A:1008202821328)|Эволюционный алгоритм, каждый вектор из поколения мутируется кроссинговером с линейной комбинацией 3 случайных векторов|
|DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization|2021|Noor Awad, Neeratyoy Mallik, Frank Hutter|[link](https://arxiv.org/abs/2105.09821)|Комбинация DE и идеи HyperBand: в каждой итерации внутри bracket новые поколения предлагаются с помощью DE и вычисляются с увеличенным бюджетом, при этом векторы для мутации сэмплируются не из текущего поколения, а из лучших векторов предыдущей SH bracket|