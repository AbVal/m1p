\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Увеличение эффективности подбора гиперпараметров}

\author{ Валентин А. Абрамов \\
	Факультет вычислительной математики и кибернетики\\
	МГУ имени М. В. Ломоносова\\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

% \renewcommand{\shorttitle}{\textit{arXiv} Template}
% \renewcommand{\abstractname}{Аннотация}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\begin{abstract}
В данной статье предложено улучшение метода дифференциальной эволюции, лежащего в основе DEHB. Алгоритм DEHB жадный -- постоянно переиспользует старые значения. Предложено добавлять шум во время мутации для увеличения покрытия пространства гиперпараметров.
Были представлены экспериментальные результаты, демонстрирующие эффективность этого метода в сравнении с базовым DEHB и традиционными методами выбора гиперпараметров. Результаты показывают, что изменение мутации может обеспечить более высокую точность модели при неизменном использовании вычислительных ресурсов.

\end{abstract}

\keywords{Оптимизация гиперпараметров \and Эволюционный алгоритм}

\section{Введение}


В последние годы машинное обучение стало одной из самых активно развивающихся областей в информационных технологиях. Чтобы достичь высокого качества моделей машинного обучения, необходимо правильно настроить гиперпараметры алгоритмов.

Гиперпараметры являются важными параметрами моделей машинного обучения, которые не могут быть определены в процессе обучения. Традиционные методы выбора гиперпараметров, такие как сеточный поиск \cite{grid_search} или случайный поиск \cite{random_search}, могут быть неэффективными или требовать значительных вычислительных ресурсов, поэтому вместо них используют более сложные алгоритмы, например, эволюционные. Одним из таких алгоритмов является DEHB \cite{awad2021dehb}.


\section{Обзор литературы}
Гиперпараметрическая оптимизация - большая и перспективная область исследований. В статье \cite{bischl2021hyperparameter} приводится подробный обзор существующих методов, все алгоритмы формально описаны, поднимаются вопросы переобучения гиперпараметров на валидационную выборку и параллелизации алгоритмов. Для глубоких нейронных сетей также необходимо подбирать гиперпараметры, такие как число слоёв, learning rate и даже связи между слоями. О задачах гиперпараметрической оптимизации в нейронных сетях подробно написано в статье \cite{Talbi2020OptimizationOD}.

Задача автоматического подбора гиперпараметров активно исследуется (\cite{Liu_2023}, \cite{morales2023survey}). Ранее использовались методы перебора по сетке (\cite{grid_search}) или случайного поиска значений (\cite{random_search}), но эти методы имеют свои недостатки, такие как случайность, проклятие размерности, сложность подбора сетки, медленная скорость работы, невозможность учесть взаимосвязи между гиперпараметрами и зависимость целевой метрики от них. В 2011 году был создан подход SMBO - последовательная оптимизация, основанная на суррогатной функции (\cite{10.1007/978-3-642-25566-3_40}). В качестве суррогатной функции изначально брали гауссовские процессы (\cite{wistuba2018scalable}), но они подходят не для всех задач, так как предназначены только для непрерывных гиперпараметров. Также был придуман алгоритм Succesive Halving (\cite{jamieson2015nonstochastic}), представляющий из себя жадный алгоритм, инкрементально выделяющий больше ресурсов для оценки гиперпараметров с хорошим потенциалом.

На данный момент одним из лучших методов поиска является TPE (\cite{NIPS2011_86e8f7ab}), основанный на байесовском подходе. Также существуют методы, основанные на подходе SMBO, такие как SMAC (\cite{10.1007/978-3-642-25566-3_40}, \cite{lindauer2022smac3}), использующий случайный лес в качестве суррогата, или модели, использующие суррогат на основе гауссовских процессов (\cite{schilling2016scalable}, \cite{wistuba2018scalable}). Также стоит упомянуть об эволюционных алгоритмах (\cite{young2015optimizing}, \cite{alibrahim2021hyperparameter}), так как они позволяют оптимизировать любые функции за счет интеллектуального перебора значений, основанного на таких идеях эволюции, как мутация, селекция и естественный отбор. Лучше всего себя показывают методы, основанные на алгоритме Succesive Halving -- они позволяют найти лучший набор гиперпараметров в условиях ограниченности вычислительных мощностей. На основе SH созданы алгоритмы HyperBand (\cite{li2018hyperband}), автоматически выделяющий ресурсы для нескольких запусков SH, BOHB (\cite{falkner2018bohb}), добавляющий в HyperBand суррогат на основе TPE, и DEHB \cite{awad2021dehb}, добавляющий в HB идею дифференциальной эволюции. 

По текущим бенчмаркам (\cite{vanschoren2014openml}, \cite{zela2020bench}) DEHB является SOTA-алгоритмом подбора гиперпараметров, но у него есть недостатки. Из-за жадного переиспользования оптимальных значений при дифференциальной эволюции, модель застревает в локальном минимуме и целевая метрика выходит на плато. Чтобы этого избежать, необходимо перейти от эксплуатации оптимальных в смысле локального минимума значений и добавить новых кандидатов, отличающихся от эксплуатируемых. 

Предлагается добавлять шумовые векторы в эволюционном шаге. Размер шума может зависеть от количества итераций, меняться от выхода на плато по целевой метрике или быть постоянным как гиперпараметр. Таким образом, благодаря шумовым векторам увеличится покрытие поверхности функции ошибки от гиперпараметров.

Для демонстрации результатов работы предложенного метода будут использованы датасеты с сайта \href{https://www.openml.org/}{open-ml}.


\section{Постановка задачи}
Задача гиперпараметрической оптимизации состоит в том, чтобы имея набор данных $X \in \mathbb{R}^{N\times d}, y \in \mathbb{R}^{N}$, пространство гиперпараметров $\Theta \in \mathbb{R}^k$, модель $f(\theta): \mathbb{R}^{d} \to \mathbb{R},\ \theta \in \Theta$, $f \in \mathbb{H}$ и целевую метрику $c(f, X, y): \mathbb{H} \times \mathbb{R}^{N\times d} \times \mathbb{R}^{N} \to \mathbb{R}$, найти $\theta^*$ такую, что выполнено
$$\theta^* = \argmin_{\theta} c(f(\theta), X, y)$$
Таким образом, зная модель $f$, пространство гиперпараметров $\Theta$, набор данных $X, y$ и целевую метрику $c$, необходимо найти набор гиперпараметров $\theta^*$, при котором целевая метрика $c$ наименьшая при обучении модели $f$ на данных $X, y$.

Предполагается, что набор данных состоит из непрерывных переменных, функция $c$ имеет бюджет вычислений. Им может быть ограничение на максимальное число итераций при обучении нейронной сети, количество деревьев в случайном лесе и градиентном бустинге и т. п.


\section{Базовый метод}
\subsection{Differential Evolution}
Алгоритм дифференциальной эволюции (\cite{de}) -- эволюционный алгоритм для наборов непрерывных переменных. Поколение $g$ состоит из $N$ кандидатов $x_{i, g} = (x_{i,g}^1, x_{i,g}^2, ..., x_{i,g}^D)$. Первое поколение инициализируется кандидатами, сэмплируемыми из равномерного распределения. Для каждого кандидата вычисляется целевая функция $f$. Далее для каждого кандидата $x_{i, g}$ выбираются 3 случайных кандидата  $x_{r_1, g}, x_{r_2, g}, x_{r_3, g}$ и строится вектор-мутант вида $v_{i, g} = x_{r_1, g} + F(x_{r_2, g} - x_{r_3, g})$, $F$ -- число-гиперпараметр метода. Далее на основе объектов $x_{i, g}$ и $v_{i, g}$ строится новый кандидат-отпрыск $u_{i, g}$ по следующей схеме: в j-ой координате нового вектора с вероятностью $p$ выбирается значение $x_{i, g}^j$ и с вероятностью $1 - p$ значение $v_{i, g}^j$, $p$ -- гиперпараметр. Этот шаг называется кроссовером (или кроссинговером). Далее для всех отпрысков вычисляется целевая функция и в качестве следующей популяции выбираются N лучших (в смысле значений целевой функции) кандидатов из всех $x_{i, g}$ и $u_{i, g}$.

\subsection{Succesive Halving}
Succesive Halving (\cite{jamieson2015nonstochastic}) -- алгоритм подбора гиперпараметров, подразумевающий наличие бюджета вычислений. То есть целевую функцию $f$ можно вычислить с фиксированным бюджетом, например, можно ограничить количество итераций вычисления $f$. У алгоритма есть три гиперпараметра: количество начальных кандидатов $N$, минимальный бюджет $\lambda$ и $\eta$ -- параметр скалирования. На первой итерации из равномерного распределения сэмплируются $N$ кандидатов, для них вычисляется функция $f$ с бюджетом $\lambda$. На второй итерации берутся $N/\eta$ лучших кандидатов, для них вычисляется $f$ с бюджетом $\eta\lambda$. На i-ой итерации функция $f$ вычисляется для $N/\eta^{i - 1}$ лучших кандидатов с шага $i - 1$ с бюджетом $\eta^{i - 1}\lambda$. Таким образом, алгоритм дает больше ресурсов перспективным кандидатам и отсекает тех, кто при низком бюджете показывает себя хуже остальных.

\subsection{HyperBand}
HyperBand (\cite{li2018hyperband}) расширяет идеи SH -- производится несколько запусков SH с различными значениями $N$ и $\lambda$. Для каждого нового запуска уменьшается число кандидатов и увеличивается минимальный бюджет. Таким образом, функции $f$, которые долго сходятся, проверяются для случайных инициализаций и с низким бюджетом, и с высоким. Главным минусов SH и HyperBand является то, что они полностью полагаются на случайное сэмплирование.

\subsection{DEHB}
DEHB -- Differential Evolution HyperBand -- развивает идеи HyperBand: сэмплирование из равномерного распределения производится один раз, далее лучшие кандидаты передаются в следующие запуски SH. На каждом шаге SH производится модифицированная дифференциальная эволюция -- текущие кандидаты, полученные из предыдущего запуска SH, скрещиваются с мутантами, полученными с помощью сэмплирования лучших кандидатов с предыдущей итерации текущего запуска SH.

\section{Предлагаемый метод}
У алгоритма DEHB есть недостаток -- он использует случайное сэмплирование всего один раз, и далее кандидаты меняются только за счет дифференциальной эволюции. Алгоритм часто застревает в локальном минимуме и выходит на плато по целевой метрике, что может быть связано с жадным переиспользованием кандидатов, лежащих на маленьком расстоянии друг от друга и имеющим близкие значения метрики. 

Чтобы этого избежать, предлагается добавлять шумовые векторы к мутантам при эволюции. Благодаря шумовым векторам увеличится покрытие поверхности функции ошибки от гиперпараметров. Это приведет к тому, что некоторые наборы гиперпараметров из популяций будут находиться вне локального минимума. При скрещивании с наборами вне локального минимума DEHB сможет найти новые оптимальные решения.

Таким образом, мутант будет иметь вид $v_{i, g} = x_{r_1, g} + F(x_{r_2, g} - x_{r_3, g}) + m(t)\varepsilon$, где $m(t): \mathbb{R} \to \mathbb{R}$ -- множитель при шумовом векторе. $m(t)$ может зависеть от номера итерации и увеличиваться со временем, может зависеть от выхода на плато, и также может быть константой.

\section{Вычислительные эксперименты [WORK IN PROGRESS]}

\subsection{Исходные данные и условия эксперимента}
Гиперпараметр предложенного метода -- функция $m(t)$, задающая уровень шума в зависимости от события (номера итерации или номера итерации после выхода на плато).

Эксперименты проведены на реализации DEHB из библиотеки auto-ml. 
Метод тестируется на  данных о недвижимости California Housing из библиотеки sklearn и реальных наборах данных с сайта UCI Machine Learning Repository: \href{https://archive.ics.uci.edu/ml/datasets/Gas+Turbine+CO+and+NOx+Emission+Data+Set#}{Gas Turbine}, \href{https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data}{Beijing PM2.5}, \href{https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+}{Grid Stability}, \href{https://archive.ics.uci.edu/ml/datasets/Bias+correction+of+numerical+prediction+model+temperature+forecast}{Temperature Forecast}.

Наборы данных разбивались на обучающую, тестовую и валидационную выборку в соотношении 7:2:1. Оптимизируемый функционал ошибки -- MSE, для сравнения моделей также используется MAE. 

\section{Выводы}
Таким образом, предложенный метод показывает улучшение на некоторых датасетах. Добавление шума при выходе на плато позволяет получить новых кандидатов и продолжить подбор гиперпараметров с помощью дифференциальной эволюции.

Главной особенностью нового метода является гиперпараметр $m(t)$. Если взять его равным 0, то метод сводится к обычному DEHB. От выбора функции $m(t)$ зависит соотношение exploration/exploitation. При большим уровнях шума мутанты получаются полностью случайными, а при низких -- слишком близкими к кандидатам. Это одновременно является плюсом и минусом метода -- при удачно подобранном $m(t)$ алгоритм сможет быстро уходить с плато, а при неудачном -- покажет себя хуже классического DEHB.

Дальнейшими направлениями исследования являются изучение возможности добавления сэмплирования кандидатов из неисследованных участков пространства гиперпараметров, как это происходит в SMBO. Также можно изменить функции мутации -- например, сдвигать мутанта в направлении увеличения качества целевой метрики. Функцию кроссовера также можно сделать более сложной -- не выбирать одно из двух значений, а брать промежуточное.





% \label{sec:headings}

% \lipsum[4] See Section \ref{sec:headings}.

% \subsection{Headings: second level}
% \lipsum[5]
% \begin{equation}
% 	\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
% \end{equation}

% \subsubsection{Headings: third level}
% \lipsum[6]

% \paragraph{Paragraph}
% \lipsum[7]



% \section{Examples of citations, figures, tables, references}
% \label{sec:others}

% \subsection{Citations}
% Citations use \verb+natbib+. The documentation may be found at
% \begin{center}
% 	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}

% Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

% \subsection{Figures}
% \lipsum[10]
% See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
% \lipsum[11]

% \begin{figure}
% 	\centering
% 	\includegraphics[width=0.5\textwidth]{../figures/log_reg_cs_exp.eps}
% 	\caption{Sample figure caption.}
% 	\label{fig:fig1}
% \end{figure}

% \subsection{Tables}
% See awesome Table~\ref{tab:table}.

% The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
% \begin{center}
% 	\url{https://www.ctan.org/pkg/booktabs}
% \end{center}


% \begin{table}
% 	\caption{Sample table title}
% 	\centering
% 	\begin{tabular}{lll}
% 		\toprule
% 		\multicolumn{2}{c}{Part}                   \\
% 		\cmidrule(r){1-2}
% 		Name     & Description     & Size ($\mu$m) \\
% 		\midrule
% 		Dendrite & Input terminal  & $\sim$100     \\
% 		Axon     & Output terminal & $\sim$10      \\
% 		Soma     & Cell body       & up to $10^6$  \\
% 		\bottomrule
% 	\end{tabular}
% 	\label{tab:table}
% \end{table}

% \subsection{Lists}
% \begin{itemize}
% 	\item Lorem ipsum dolor sit amet
% 	\item consectetur adipiscing elit.
% 	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
% \end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}