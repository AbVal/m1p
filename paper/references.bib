@inproceedings{NIPS2011_86e8f7ab,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}

@InProceedings{10.1007/978-3-642-25566-3_40,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
booktitle="Learning and Intelligent Optimization",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
abstract="State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.",
isbn="978-3-642-25566-3"
}

@misc{li2018hyperband,
      title={Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization}, 
      author={Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
      year={2018},
      eprint={1603.06560},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{klein2017fast,
      title={Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}, 
      author={Aaron Klein and Stefan Falkner and Simon Bartels and Philipp Hennig and Frank Hutter},
      year={2017},
      eprint={1605.07079},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{snoek2015scalable,
      title={Scalable Bayesian Optimization Using Deep Neural Networks}, 
      author={Jasper Snoek and Oren Rippel and Kevin Swersky and Ryan Kiros and Nadathur Satish and Narayanan Sundaram and Md. Mostofa Ali Patwary and Prabhat and Ryan P. Adams},
      year={2015},
      eprint={1502.05700},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{falkner2018bohb,
      title={BOHB: Robust and Efficient Hyperparameter Optimization at Scale}, 
      author={Stefan Falkner and Aaron Klein and Frank Hutter},
      year={2018},
      eprint={1807.01774},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{article,
author = {Storn, Rainer and Price, Kenneth},
year = {1997},
month = {01},
pages = {341-359},
title = {Differential Evolution - A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces},
volume = {11},
journal = {Journal of Global Optimization},
doi = {10.1023/A:1008202821328}
}

@misc{awad2021dehb,
      title={DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization}, 
      author={Noor Awad and Neeratyoy Mallik and Frank Hutter},
      year={2021},
      eprint={2105.09821},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jamieson2015nonstochastic,
      title={Non-stochastic Best Arm Identification and Hyperparameter Optimization}, 
      author={Kevin Jamieson and Ameet Talwalkar},
      year={2015},
      eprint={1502.07943},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bischl2021hyperparameter,
      title={Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges}, 
      author={Bernd Bischl and Martin Binder and Michel Lang and Tobias Pielok and Jakob Richter and Stefan Coors and Janek Thomas and Theresa Ullmann and Marc Becker and Anne-Laure Boulesteix and Difan Deng and Marius Lindauer},
      year={2021},
      eprint={2107.05847},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Talbi2020OptimizationOD,
  title={Optimization of deep neural networks: a survey and unified taxonomy},
  author={El-Ghazali Talbi},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:219427357}
}