@inproceedings{NIPS2011_86e8f7ab,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}

@InProceedings{10.1007/978-3-642-25566-3_40,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
booktitle="Learning and Intelligent Optimization",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
abstract="State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.",
isbn="978-3-642-25566-3"
}

@misc{li2018hyperband,
      title={Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization}, 
      author={Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
      year={2018},
      eprint={1603.06560},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{klein2017fast,
      title={Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}, 
      author={Aaron Klein and Stefan Falkner and Simon Bartels and Philipp Hennig and Frank Hutter},
      year={2017},
      eprint={1605.07079},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{snoek2015scalable,
      title={Scalable Bayesian Optimization Using Deep Neural Networks}, 
      author={Jasper Snoek and Oren Rippel and Kevin Swersky and Ryan Kiros and Nadathur Satish and Narayanan Sundaram and Md. Mostofa Ali Patwary and Prabhat and Ryan P. Adams},
      year={2015},
      eprint={1502.05700},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{falkner2018bohb,
      title={BOHB: Robust and Efficient Hyperparameter Optimization at Scale}, 
      author={Stefan Falkner and Aaron Klein and Frank Hutter},
      year={2018},
      eprint={1807.01774},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{de,
author = {Storn, Rainer and Price, Kenneth},
year = {1997},
month = {01},
pages = {341-359},
title = {Differential Evolution - A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces},
volume = {11},
journal = {Journal of Global Optimization},
doi = {10.1023/A:1008202821328}
}

@misc{awad2021dehb,
      title={DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization}, 
      author={Noor Awad and Neeratyoy Mallik and Frank Hutter},
      year={2021},
      eprint={2105.09821},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jamieson2015nonstochastic,
      title={Non-stochastic Best Arm Identification and Hyperparameter Optimization}, 
      author={Kevin Jamieson and Ameet Talwalkar},
      year={2015},
      eprint={1502.07943},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bischl2021hyperparameter,
      title={Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges}, 
      author={Bernd Bischl and Martin Binder and Michel Lang and Tobias Pielok and Jakob Richter and Stefan Coors and Janek Thomas and Theresa Ullmann and Marc Becker and Anne-Laure Boulesteix and Difan Deng and Marius Lindauer},
      year={2021},
      eprint={2107.05847},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Talbi2020OptimizationOD,
  title={Optimization of deep neural networks: a survey and unified taxonomy},
  author={El-Ghazali Talbi},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:219427357}
}

@article{random_search,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281--305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}


@inproceedings{grid_search,
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
title = {An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273556},
doi = {10.1145/1273496.1273556},
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {473–480},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{Liu_2023,
   title={Online Hyperparameter Optimization for Class-Incremental Learning},
   volume={37},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v37i7.26070},
   DOI={10.1609/aaai.v37i7.26070},
   number={7},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificia\cite{random_search}l Intelligence (AAAI)},
   author={Liu, Yaoyao and Li, Yingying and Schiele, Bernt and Sun, Qianru},
   year={2023},
   month=jun, pages={8906–8913} }

@article{morales2023survey,
  title={A survey on multi-objective hyperparameter optimization algorithms for machine learning},
  author={Morales-Hern{\'a}ndez, Alejandro and Van Nieuwenhuyse, Inneke and Rojas Gonzalez, Sebastian},
  journal={Artificial Intelligence Review},
  volume={56},
  number={8},
  pages={8043--8093},
  year={2023},
  publisher={Springer}
}

@article{wistuba2018scalable,
  title={Scalable gaussian process-based transfer surrogates for hyperparameter optimization},
  author={Wistuba, Martin and Schilling, Nicolas and Schmidt-Thieme, Lars},
  journal={Machine Learning},
  volume={107},
  number={1},
  pages={43--78},
  year={2018},
  publisher={Springer}
}

@article{lindauer2022smac3,
  title={SMAC3: A versatile Bayesian optimization package for hyperparameter optimization},
  author={Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr{\'e} and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, Ren{\'e} and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={2475--2483},
  year={2022},
  publisher={JMLRORG}
}

@article{wistuba2018scalable,
  title={Scalable gaussian process-based transfer surrogates for hyperparameter optimization},
  author={Wistuba, Martin and Schilling, Nicolas and Schmidt-Thieme, Lars},
  journal={Machine Learning},
  volume={107},
  number={1},
  pages={43--78},
  year={2018},
  publisher={Springer}
}

@inproceedings{schilling2016scalable,
  title={Scalable hyperparameter optimization with products of gaussian process experts},
  author={Schilling, Nicolas and Wistuba, Martin and Schmidt-Thieme, Lars},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16},
  pages={33--48},
  year={2016},
  organization={Springer}
}

@inproceedings{alibrahim2021hyperparameter,
  title={Hyperparameter optimization: Comparing genetic algorithm against grid search and bayesian optimization},
  author={Alibrahim, Hussain and Ludwig, Simone A},
  booktitle={2021 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1551--1559},
  year={2021},
  organization={IEEE}
}

@inproceedings{young2015optimizing,
  title={Optimizing deep learning hyper-parameters through an evolutionary algorithm},
  author={Young, Steven R and Rose, Derek C and Karnowski, Thomas P and Lim, Seung-Hwan and Patton, Robert M},
  booktitle={Proceedings of the workshop on machine learning in high-performance computing environments},
  pages={1--5},
  year={2015}
}

@article{vanschoren2014openml,
  title={OpenML: networked science in machine learning},
  author={Vanschoren, Joaquin and Van Rijn, Jan N and Bischl, Bernd and Torgo, Luis},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={15},
  number={2},
  pages={49--60},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@article{zela2020bench,
  title={Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search},
  author={Zela, Arber and Siems, Julien and Hutter, Frank},
  journal={arXiv preprint arXiv:2001.10422},
  year={2020}
}